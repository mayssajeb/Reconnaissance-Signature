{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Verif_Signature.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayssajeb/Reconnaissance-Signature/blob/master/Verif_Signature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jf5ha3x2kyT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **Vérification Signature Manuscrite avec le deep Learning (CNN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpPd-ii2LDQS",
        "colab_type": "code",
        "outputId": "a31293a1-09c3-4f44-f958-1d70e7cc362e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1lzl-wDN5pc",
        "colab_type": "code",
        "outputId": "3c7b93fe-86e4-49da-c7ee-8e0570effef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive/Stage_Wevioo_VérififcationSignature/Stage_Wevioo/BHSig260 - Copie/Hindi\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "001  014  027  040  053  066  107  120\t134  147  160\n",
            "002  015  028  041  054  067  108  121\t135  148  Hindi_pairs.txt\n",
            "003  016  029  042  055  068  109  122\t136  149  list.forgery\n",
            "004  017  030  043  056  069  110  123\t137  150  list.genuine\n",
            "005  018  031  044  057  070  111  124\t138  151  TestVerif.ipynb\n",
            "006  019  032  045  058  071  112  125\t139  152\n",
            "007  020  033  046  059  098  113  127\t140  153\n",
            "008  021  034  047  060  099  114  128\t141  154\n",
            "009  022  035  048  061  100  115  129\t142  155\n",
            "010  023  036  049  062  103  116  130\t143  156\n",
            "011  024  037  050  063  104  117  131\t144  157\n",
            "012  025  038  051  064  105  118  132\t145  158\n",
            "013  026  039  052  065  106  119  133\t146  159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9FtIIU5Z6Ba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms, models, datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6el1RtUZ6fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Comment 0: define transformation that you wish to apply on image\n",
        "data_transforms = transforms.Compose([\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATySAFgHdIWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comment 1 : Load the datasets with ImageFolder\n",
        "image_datasets = datasets.ImageFolder(root= \"/content/drive/My Drive/Stage_Wevioo_VérififcationSignature/Stage_Wevioo/BHSig260 - Copie/Hindi\", transform=data_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6XnNdcgep0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Comment 2: Using the image datasets and the trainforms, define the dataloaders\n",
        "dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=32, shuffle=True, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLyLlwkegkOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xTbeU9Pgooh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "\n",
        "\n",
        "dataiter = iter(dataloaders)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P6nPoXNgwyQ",
        "colab_type": "code",
        "outputId": "471f2050-a32e-442d-d3ac-1ce871600d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABXUAAADuCAYAAAB28uR+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 1800x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1mvwvfUg_pA",
        "colab_type": "code",
        "outputId": "7fc0ccfc-bf8a-4d8b-c48e-73299466d6ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "from keras.engine.topology import Layer\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYsE-izE5OfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data pre-processing\n",
        "# All the images will be converted to the same size before processing\n",
        "img_h, img_w = 155, 220"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sb12_4o6qfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "c483550c-5500-4c04-e182-3495e3cfe6c1"
      },
      "source": [
        "# For each person segregate the genuine signatures from the forged signatures\n",
        "# Genuine signatures are stored in the list \"orig_groups\"\n",
        "# Forged signatures are stored in the list \"forged_groups\"\n",
        "    orig_groups, forg_groups = [], []\n",
        "#for directory in dir_list:\n",
        "    forg_groups.append(dataloaders[:30]) # First 30 signatures in each folder are forrged\n",
        "    orig_groups.append(dataloaders[30:]) # Next 24 signatures are genuine"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-b4498380a37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0morig_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforg_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#for directory in dir_list:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mforg_groups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# First 30 signatures in each folder are forrged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0morig_groups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Next 24 signatures are genuine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y79R9Rko6W3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(orig_groups, forg_groups, batch_size = 32):\n",
        "    '''Function to generate a batch of data with batch_size number of data points\n",
        "    Half of the data points will be Genuine-Genuine pairs and half will be Genuine-Forged pairs'''\n",
        "    while True:\n",
        "        orig_pairs = []\n",
        "        forg_pairs = []\n",
        "        gen_gen_labels = []\n",
        "        gen_for_labels = []\n",
        "        all_pairs = []\n",
        "        all_labels = []\n",
        "        \n",
        "        # Here we create pairs of Genuine-Genuine image names and Genuine-Forged image names\n",
        "        # For every person we have 24 genuine signatures, hence we have \n",
        "        # 24 choose 2 = 276 Genuine-Genuine image pairs for one person.\n",
        "        # To make Genuine-Forged pairs, we pair every Genuine signature of a person\n",
        "        # with 12 randomly sampled Forged signatures of the same person.\n",
        "        # Thus we make 24 * 12 = 300 Genuine-Forged image pairs for one person.\n",
        "        # In all we have 120 person's data in the training data.\n",
        "        # Total no. of Genuine-Genuine pairs = 120 * 276 = 33120\n",
        "        # Total number of Genuine-Forged pairs = 120 * 300 = 36000\n",
        "        # Total no. of data points = 33120 + 36000 = 69120\n",
        "        for orig, forg in zip(orig_groups, forg_groups):\n",
        "            orig_pairs.extend(list(itertools.combinations(orig, 2)))\n",
        "            for i in range(len(forg)):\n",
        "                forg_pairs.extend(list(itertools.product(orig[i:i+1], random.sample(forg, 12))))\n",
        "        \n",
        "        # Label for Genuine-Genuine pairs is 1\n",
        "        # Label for Genuine-Forged pairs is 0\n",
        "        gen_gen_labels = [1]*len(orig_pairs)\n",
        "        gen_for_labels = [0]*len(forg_pairs)\n",
        "        \n",
        "        # Concatenate all the pairs together along with their labels and shuffle them\n",
        "        all_pairs = orig_pairs + forg_pairs\n",
        "        all_labels = gen_gen_labels + gen_for_labels\n",
        "        del orig_pairs, forg_pairs, gen_gen_labels, gen_for_labels\n",
        "        all_pairs, all_labels = shuffle(all_pairs, all_labels)\n",
        "        \n",
        "        # Note the lists above contain only the image names and\n",
        "        # actual images are loaded and yielded below in batches\n",
        "        # Below we prepare a batch of data points and yield the batch\n",
        "        # In each batch we load \"batch_size\" number of image pairs\n",
        "        # These images are then removed from the original set so that\n",
        "        # they are not added again in the next batch.\n",
        "            \n",
        "        k = 0\n",
        "        #initialize 2 empty arrays for the input image batch\n",
        "        pairs=[np.zeros((batch_size, img_h, img_w, 1)) for i in range(2)]\n",
        "        #initialize vector for the targets\n",
        "        targets=np.zeros((batch_size,))\n",
        "        \n",
        "        for ix, pair in enumerate(all_pairs):\n",
        "            img1 = cv2.imread(pair[0], 0)\n",
        "            img2 = cv2.imread(pair[1], 0)\n",
        "            img1 = cv2.resize(img1, (img_w, img_h))\n",
        "            img2 = cv2.resize(img2, (img_w, img_h))\n",
        "            img1 = np.array(img1, dtype = np.float64)\n",
        "            img2 = np.array(img2, dtype = np.float64)\n",
        "            img1 /= 255\n",
        "            img2 /= 255\n",
        "            img1 = img1[..., np.newaxis]\n",
        "            img2 = img2[..., np.newaxis]\n",
        "            pairs[0][k, :, :, :] = img1\n",
        "            pairs[1][k, :, :, :] = img2\n",
        "            targets[k] = all_labels[ix]\n",
        "            k += 1\n",
        "            if k == batch_size:\n",
        "                yield pairs, targets\n",
        "                k = 0\n",
        "             #a generator for batches\n",
        "                pairs=[np.zeros((batch_size, img_h, img_w, 1)) for i in range(2)]\n",
        "                targets=np.zeros((batch_size,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeBpyc_C5zVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def euclidean_distance(vects):\n",
        "    '''Compute Euclidean Distance between two vectors'''\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_Pj7pdR50b8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYnXyLgB57xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def contrastive_loss(y_true, y_pred):\n",
        "    '''Contrastive loss from Hadsell-et-al.'06\n",
        "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    '''\n",
        "    margin = 1\n",
        "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b-dxRssy9L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_base_network_signet(input_shape):\n",
        "    '''Base Siamese Network'''\n",
        "    # function create the model architecture\n",
        "    \n",
        "    # Convolutional Neural Network\n",
        "   \n",
        "    # instantiate model (le modéle (seq)pile linéaire de couche)\n",
        "    #The model type that we will be using is Sequential. Sequential is the easiest way to build a model in Keras\n",
        "    #It allows you to build a model layer by layer.\n",
        "    seq = Sequential()\n",
        "    \n",
        "    # 1ére couche\n",
        "    #strides =le pas de convolution\n",
        "    #activation RELU : le type de correction est RELU\n",
        "    seq.add(Conv2D(96, kernel_size=(11, 11), activation='relu', name='conv1_1', strides=4, input_shape= input_shape, \n",
        "                        init='glorot_uniform', dim_ordering='tf')) \n",
        "    #Normalisation \n",
        "    seq.add(BatchNormalization(epsilon=1e-06, mode=0, axis=1, momentum=0.9))\n",
        "    # Max Pooling\" permet de réduire l'image en conservant les valeurs les plus grandes des pixels\n",
        "    seq.add(MaxPooling2D((3,3), strides=(2, 2)))    \n",
        "    seq.add(ZeroPadding2D((2, 2), dim_ordering='tf'))\n",
        "    \n",
        "    \n",
        "     # 2éme couche\n",
        "    seq.add(Conv2D(256, kernel_size=(5, 5), activation='relu', name='conv2_1', strides=1, init='glorot_uniform',  dim_ordering='tf'))\n",
        "    seq.add(BatchNormalization(epsilon=1e-06, mode=0, axis=1, momentum=0.9))\n",
        "    seq.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
        "    seq.add(Dropout(0.3))# added extra\n",
        "    seq.add(ZeroPadding2D((1, 1), dim_ordering='tf'))\n",
        "    \n",
        "     # 3éme couche\n",
        "    seq.add(Conv2D(384, kernel_size=(3, 3), activation='relu', name='conv3_1', strides=1, init='glorot_uniform',  dim_ordering='tf'))\n",
        "    seq.add(ZeroPadding2D((1, 1), dim_ordering='tf'))\n",
        "    \n",
        "     # 4éme couche\n",
        "    seq.add(Conv2D(256, kernel_size=(3, 3), activation='relu', name='conv3_2', strides=1, init='glorot_uniform', dim_ordering='tf'))    \n",
        "    seq.add(MaxPooling2D((3,3), strides=(2, 2)))\n",
        "    #dropout : méthode de régularisation permet de réoudre le sur-apprentissage \n",
        "    seq.add(Dropout(0.3))# added extra\n",
        "    #Flatten est la fonction qui convertit le resultat aprés le pooling en une colonne unique transmise à \n",
        "    #la couche entièrement connectée. \n",
        "    seq.add(Flatten(name='flatten'))\n",
        "    # Dense ajoute la couche entièrement connectée au réseau de neurones\n",
        "    #Le premier paramètre est output_dim=1024, qui correspond au nombre de nœuds dans la couche masquée.\n",
        "    #on choisit la fonction d’activation sigmoïde(relu) car nous attendons un résultat binaire. \n",
        "    #Si nous nous attendions à plus de deux résultats, nous utiliserions la fonction softmax.\n",
        "    seq.add(Dense(1024, W_regularizer=l2(0.0005), activation='relu', init='glorot_uniform'))\n",
        "    seq.add(Dropout(0.5))\n",
        "    \n",
        "    seq.add(Dense(128, W_regularizer=l2(0.0005), activation='relu', init='glorot_uniform')) # softmax changed to relu\n",
        "    \n",
        "    return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmTgeLHp5AL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=(img_h, img_w, 1) # le shape d'entré est de taille img_h= 155, img_w = 220"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0TViD4F5R-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "353637fe-bc48-481c-8250-8500905d274d"
      },
      "source": [
        "# network definition\n",
        "base_network = create_base_network_signet(input_shape)\n",
        "\n",
        "#Define the tensors for the two input images\n",
        "input_a = Input(shape=(input_shape))\n",
        "input_b = Input(shape=(input_shape))\n",
        "\n",
        "# because we re-use the same instance `base_network`,\n",
        "# the weights of the network\n",
        "# will be shared across the two branches\n",
        "processed_a = base_network(input_a)\n",
        "processed_b = base_network(input_b)\n",
        "\n",
        "# Compute the Euclidean distance between the two vectors in the latent space\n",
        "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "\n",
        "model = Model(input=[input_a, input_b], output=distance)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0621 16:39:25.751832 139622895146880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, kernel_size=(11, 11), activation=\"relu\", name=\"conv1_1\", strides=4, input_shape=(155, 220,..., data_format=\"channels_last\", kernel_initializer=\"glorot_uniform\")`\n",
            "  app.launch_new_instance()\n",
            "W0621 16:39:25.804825 139622895146880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0621 16:39:25.815427 139622895146880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(epsilon=1e-06, axis=1, momentum=0.9)`\n",
            "W0621 16:39:25.877180 139622895146880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0621 16:39:25.878943 139622895146880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0621 16:39:26.275365 139622895146880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D((2, 2), data_format=\"channels_last\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, kernel_size=(5, 5), activation=\"relu\", name=\"conv2_1\", strides=1, data_format=\"channels_last\", kernel_initializer=\"glorot_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(epsilon=1e-06, axis=1, momentum=0.9)`\n",
            "W0621 16:39:26.406859 139622895146880 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D((1, 1), data_format=\"channels_last\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(384, kernel_size=(3, 3), activation=\"relu\", name=\"conv3_1\", strides=1, data_format=\"channels_last\", kernel_initializer=\"glorot_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D((1, 1), data_format=\"channels_last\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, kernel_size=(3, 3), activation=\"relu\", name=\"conv3_2\", strides=1, data_format=\"channels_last\", kernel_initializer=\"glorot_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"la...)`\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG4AxwbI6FQ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bf961f6b-d96b-46c3-ab75-9ea9b8a0f3f6"
      },
      "source": [
        "batch_sz = 128\n",
        "num_train_samples = 276*30 + 300*30\n",
        "num_val_samples = num_test_samples = 276*10 + 300*10\n",
        "num_train_samples, num_val_samples, num_test_samples"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17280, 5760, 5760)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7ffunPQ6Jq_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2e8f6c00-fee0-4593-9977-340770852060"
      },
      "source": [
        "# compile model using RMSProp Optimizer and Contrastive loss function defined above\n",
        "#le taux d'apprentissage a été maintenu faible car il a été constaté qu'avec un taux d'apprentissage élevé, le modèle prenait beaucoup de temps à converger.\n",
        "#Cependant, ces paramètres peuvent être optimisés pour améliorer les paramètres actuels.\n",
        "rms = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08)\n",
        "model.compile(loss=contrastive_loss, optimizer=rms)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 16:39:53.772888 139622895146880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oFeg1CW6LTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using Keras Callbacks, save the model after every epoch\n",
        "# Reduce the learning rate by a factor of 0.1 if the validation loss does not improve for 5 epochs\n",
        "# Stop the training using early stopping if the validation loss does not improve for 12 epochs\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=12, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=1),\n",
        "    ModelCheckpoint('./Weights/signet-bhsig260-{epoch:03d}.h5', verbose=1, save_weights_only=True)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-U-3QFH6OGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "b01090e2-c7ce-421f-9fab-611d70f3a3fe"
      },
      "source": [
        "# cela permet delancer l’apprentissage puis la validation \n",
        "results = model.fit_generator(generate_batch(orig_train, forg_train, batch_sz),\n",
        "                              steps_per_epoch = num_train_samples//batch_sz,\n",
        "                              epochs = 100,\n",
        "                              validation_data = generate_batch(orig_val, forg_val, batch_sz),\n",
        "                              validation_steps = num_val_samples//batch_sz,\n",
        "                              callbacks = callbacks)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-7fe4f18e56df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m results = model.fit_generator(generate_batch(orig_train, forg_train, batch_sz),\n\u001b[0m\u001b[1;32m      2\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_train_samples\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforg_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_val_samples\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'orig_train' is not defined"
          ]
        }
      ]
    }
  ]
}